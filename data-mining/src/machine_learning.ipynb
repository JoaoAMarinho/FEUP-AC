{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**descriptive**\n",
    "- diversity of algorithms\n",
    "- parameter tuning\n",
    "- understanding algorithm behavior\t\n",
    "- performance measure\t\n",
    "- correct interpretation of performance measures\t\n",
    "- comparative analysis of results\t\n",
    "- model improvement\t\n",
    "- analysis of results\t\n",
    "\n",
    "**predictive**\n",
    "- diversity of tasks\t\n",
    "- diversity of algorithms\t\n",
    "- parameter tuning\t\n",
    "- understanding algorithm behavior\t\n",
    "- (performance estimation) training vs test\t\n",
    "- (perfomance estimation): other factors (e.g. time)\t\n",
    "- (performance estimation): perfomance measure\t\n",
    "- (performance estimation): correct interpretation of performance measures\t\n",
    "- (performance estimation): analysis of results\t\n",
    "- model improvement\t\n",
    "- feature importance\t\n",
    "- analysis of \"white-box\" models\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ml models imports\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "# metrics imports\n",
    "from sklearn.metrics import \\\n",
    "    accuracy_score, \\\n",
    "    confusion_matrix, \\\n",
    "    log_loss, \\\n",
    "    roc_auc_score, \\\n",
    "    precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('', delimiter=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test, y_pred, elapsed_time):\n",
    "    \"\"\"\n",
    "    Measure the performance of a model by evaluating it against specific metrics\n",
    "    Arguments:\n",
    "        model: machine learning model\n",
    "        X_train: train subset used by the model in the fit step\n",
    "        X_test: test subset used by the model in the predict step\n",
    "        y_train: subset containing the real target values used by the model in the fit step\n",
    "        y_test: subset containing the real target values the model was supposed to predict in the predict step\n",
    "        y_pred: subset containing the values predicted by the model in the predict step\n",
    "        elapsed_time: time that the model took to fit and predict, in seconds \n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "interesting parameters to tune:\n",
    "    - criterion (function used to measure the quality of a split): gini (default), entropy or log_loss\n",
    "    - splitter (strategy used to choose a split): best (default) or random\n",
    "    - random_state (controls the randomness of the estimator)\n",
    "\"\"\"\n",
    "\n",
    "decision_tree_clf = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "interesting parameters to tune:\n",
    "    - max_iter (maximum number of iterations taken for the solvers to converge, default 100)\n",
    "    - solver (algorithm to use in the optimization problem): lbfgs (default)\n",
    "\n",
    "some notes on the solver:\n",
    "    - for small datasets, liblinear is a good choice, whereas sag and saga are faster for large ones;\n",
    "    - for multiclass problems, only newton-cg, sag, saga and lbfgs handle multinomial loss;\n",
    "    - liblinear is limited to one-versus-rest schemes.\n",
    "\"\"\"\n",
    "\n",
    "logistic_regression_clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "interesting parameters to tune:\n",
    "    - \n",
    "\"\"\"\n",
    "\n",
    "gaussian_nb_clf = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "interesting parameters to tune:\n",
    "    - \n",
    "\"\"\"\n",
    "\n",
    "nearest_neighbors_clf = NearestNeighbors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "interesting parameters to tune:\n",
    "    - n_estimators (number of trees in the forest, default 100)\n",
    "    - criterion (function used to measure the quality of a split): gini (default), entropy or log_loss\n",
    "    - max_depth (maximum depth of the tree, default None)\n",
    "    - n_jobs\n",
    "\"\"\"\n",
    "\n",
    "random_forest_clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "interesting parameters to tune:\n",
    "    - \n",
    "\"\"\"\n",
    "\n",
    "gradient_boosting_clf = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "interesting parameters to tune:\n",
    "    - \n",
    "\"\"\"\n",
    "\n",
    "xgb_clf = XGBClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LGBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "interesting parameters to tune:\n",
    "    - \n",
    "\"\"\"\n",
    "\n",
    "lgbm_clf = lgb.LGBMClassifier()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
