{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# import lightgbm as lgb\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import utils.ml_utils as mlu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/clean/df-train.csv\")\n",
    "amount_outliers_df = pd.read_csv(\"../data/clean/df-amount_outliers.csv\")\n",
    "age_loan_outliers_df = pd.read_csv(\"../data/clean/df-adult_loans.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply(\n",
    "    df,\n",
    "    model_instance,\n",
    "    parameter_grid,\n",
    "    cross_validation=StratifiedKFold(n_splits=5),\n",
    "    feature_selection=False,\n",
    "    filter=True,\n",
    "    oversample=False,\n",
    "    scaler=True,\n",
    "):\n",
    "    return mlu.apply_cv(\n",
    "        df,\n",
    "        model_instance,\n",
    "        parameter_grid,\n",
    "        cross_validation,\n",
    "        feature_selection,\n",
    "        filter,\n",
    "        oversample,\n",
    "        scaler,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description:\n",
    "Non-parametric supervised learning method (used for classification).\n",
    "The goal is to create a model that predicts a variable (status) by learning simple decision rules inferred from the data features.\n",
    "A tree can be seen as a piecewise constant approximation.\n",
    "\n",
    "Interesting parameters to tune:\n",
    "    - criterion (function used to measure the quality of a split): gini (default), entropy or log_loss\n",
    "    - splitter (strategy used to choose a split): best (default) or random\n",
    "    - random_state (controls the randomness of the estimator)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def apply_decision_tree(df):\n",
    "    parameter_grid = {\n",
    "        \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "        \"splitter\": [\"best\", \"random\"],\n",
    "        \"max_depth\": range(1, 7),\n",
    "    }\n",
    "    return (\n",
    "        apply(\n",
    "            df,\n",
    "            tree.DecisionTreeClassifier,\n",
    "            parameter_grid,\n",
    "            oversample=oversample,\n",
    "            feature_selection=feature_selection,\n",
    "        )\n",
    "        for oversample, feature_selection in (\n",
    "            (False, False),\n",
    "            (False, True),\n",
    "            (True, False),\n",
    "            (True, True),\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description:\n",
    "Logistic regression, despite its name, is a linear model for classification rather than regression.\n",
    "Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier.\n",
    "In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function.\n",
    "\n",
    "Interesting parameters to tune:\n",
    "    - max_iter (maximum number of iterations taken for the solvers to converge, default 100)\n",
    "    - solver (algorithm to use in the optimization problem): lbfgs (default)\n",
    "\n",
    "Some notes on the solver:\n",
    "    - for small datasets, liblinear is a good choice, whereas sag and saga are faster for large ones;\n",
    "    - for multiclass problems, only newton-cg, sag, saga and lbfgs handle multinomial loss;\n",
    "    - liblinear is limited to one-versus-rest schemes.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def apply_logistic_regression(df):\n",
    "    parameter_grid = {\n",
    "        \"penalty\": [\"l2\", \"none\"],\n",
    "        \"C\": [0.01, 0.05, 0.1, 0.2, 0.5, 1.0],\n",
    "        \"solver\": [\"newton-cg\", \"lbfgs\", \"sag\", \"saga\"],\n",
    "        \"class_weight\": [\"balanced\", None],\n",
    "        \"max_iter\": [500, 1000, 5000, 10000],\n",
    "    }\n",
    "    return (\n",
    "        apply(\n",
    "            df,\n",
    "            LogisticRegression,\n",
    "            parameter_grid,\n",
    "            oversample=oversample,\n",
    "            feature_selection=feature_selection,\n",
    "        )\n",
    "        for oversample, feature_selection in (\n",
    "            (False, False),\n",
    "            (False, True),\n",
    "            (True, False),\n",
    "            (True, True),\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description:\n",
    "Naive Bayes methods are a set of supervised learning algorithms based on applying Bayesâ€™ \n",
    "theorem with the \"naive\" assumption of conditional independence between every pair of features given the value of the class variable.\n",
    "\n",
    "Draw the Gaussian Distribution to each of the independent variables and apply the likelihood of each given value to the prior probability.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def apply_naive_bayes(df):\n",
    "    return (\n",
    "        apply(\n",
    "            df,\n",
    "            GaussianNB,\n",
    "            {},\n",
    "            oversample=oversample,\n",
    "            feature_selection=feature_selection,\n",
    "        )\n",
    "        for oversample, feature_selection in (\n",
    "            (False, False),\n",
    "            (False, True),\n",
    "            (True, False),\n",
    "            (True, True),\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-Nearest Neighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description:\n",
    "Neighbors-based classification is a type of instance-based learning or non-generalizing learning:\n",
    "it does not attempt to construct a general internal model, but simply stores instances of the training data.\n",
    "\n",
    "Classification is computed from a simple majority vote of the nearest neighbors of each point:\n",
    "a query point is assigned the data class which has the most representatives within the nearest neighbors of the point.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def apply_k_nearest_neighbours(df):\n",
    "    parameter_grid = {\n",
    "        \"n_neighbors\": [4, 5, 6, 7, 10, 15],\n",
    "        \"leaf_size\": [5, 10, 15, 20, 50, 100],\n",
    "        \"n_jobs\": [-1],\n",
    "        \"algorithm\": [\"auto\"],\n",
    "    }\n",
    "    return (\n",
    "        apply(\n",
    "            df,\n",
    "            KNeighborsClassifier,\n",
    "            parameter_grid,\n",
    "            oversample=oversample,\n",
    "            feature_selection=feature_selection,\n",
    "        )\n",
    "        for oversample, feature_selection in (\n",
    "            (False, False),\n",
    "            (False, True),\n",
    "            (True, False),\n",
    "            (True, True),\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description:\n",
    "A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset\n",
    " and uses averaging (votes) to improve the predictive accuracy and control over-fitting.\n",
    "The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree.\n",
    "\n",
    "Interesting parameters to tune:\n",
    "    - n_estimators (number of trees in the forest, default 100)\n",
    "    - criterion (function used to measure the quality of a split): gini (default), entropy or log_loss\n",
    "    - max_depth (maximum depth of the tree, default None)\n",
    "    - n_jobs\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def apply_random_forest(df):\n",
    "\n",
    "    parameter_grid = {\n",
    "        \"n_estimators\": [10, 50, 100, 200],\n",
    "        \"max_depth\": [5, 10, None],\n",
    "        \"n_jobs\": [-1],  # Use all cores\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "    }\n",
    "    return (\n",
    "        apply(\n",
    "            df,\n",
    "            RandomForestClassifier,\n",
    "            parameter_grid,\n",
    "            oversample=oversample,\n",
    "            feature_selection=feature_selection,\n",
    "            scaler=False,\n",
    "        )\n",
    "        for oversample, feature_selection in (\n",
    "            (False, False),\n",
    "            (False, True),\n",
    "            (True, False),\n",
    "            (True, True),\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Boosting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description:\n",
    "Gradient Tree Boosting or Gradient Boosted Decision Trees (GBDT) is a generalization of boosting to arbitrary differentiable loss functions.\n",
    "GBDT is an accurate and effective off-the-shelf procedure that can be used for both regression and classification problems\n",
    " in a variety of areas including Web search ranking and ecology.\n",
    "Similar to Random forest with the improvement from tree to tree (loss decrease)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def apply_gradient_boosting(df):\n",
    "    parameter_grid = {\n",
    "        \"n_estimators\": [int(x) for x in range(2, 14, 2)],\n",
    "        \"learning_rate\": [0.1, 0.3, 0.5, 0.7],\n",
    "        \"loss\": [\"deviance\", \"exponential\", \"log_loss\"],\n",
    "        \"criterion\": [\"friedman_mse\", \"squared_error\", \"mse\"],\n",
    "        \"min_samples_split\": [4, 6, 8],\n",
    "        \"min_samples_leaf\": [2, 4, 6],\n",
    "    }\n",
    "    return (\n",
    "        apply(\n",
    "            df,\n",
    "            GradientBoostingClassifier,\n",
    "            parameter_grid,\n",
    "            oversample=oversample,\n",
    "            feature_selection=feature_selection,\n",
    "        )\n",
    "        for oversample, feature_selection in (\n",
    "            (False, False),\n",
    "            (False, True),\n",
    "            (True, False),\n",
    "            (True, True),\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description:\n",
    "SVC separates the set of points (items) with a line or plane into different areas of the space.\n",
    "These areas allow us to predict if a given point is of a specific class or not.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def apply_svc(df):\n",
    "    parameter_grid = {\n",
    "        \"C\": [1, 10, 50],\n",
    "        \"gamma\": [0.001, 0.0001, 0.01, 1, \"scale\", \"auto\"],\n",
    "        \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "        \"degree\": [1, 2, 3, 4, 5, 6, 7],\n",
    "        \"coef0\": [0.0, 0.1, 0.3, 0.5, 0.7],\n",
    "        \"max_iter\": [1, 2, 3, 5, 7, 10],\n",
    "        \"decision_function_shape\": [\"ovo\", \"ovr\"],\n",
    "        \"class_weight\": [None, \"balanced\", dict],\n",
    "    }\n",
    "    return (\n",
    "        apply(\n",
    "            df,\n",
    "            SVC,\n",
    "            parameter_grid,\n",
    "            oversample=oversample,\n",
    "            feature_selection=feature_selection,\n",
    "        )\n",
    "        for oversample, feature_selection in (\n",
    "            (False, False),\n",
    "            (False, True),\n",
    "            (True, False),\n",
    "            (True, True),\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def apply_mlp(df):\n",
    "    parameter_grid = {\n",
    "        \"hidden_layer_sizes\": [(3, 5, 8, 13, 21, 34)],\n",
    "        \"activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
    "        \"solver\": [\"lbfgs\", \"sgd\", \"adam\"],\n",
    "        \"learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"],\n",
    "        \"nesterovs_momentum\": [True, False],\n",
    "        \"early_stopping\": [False, True],\n",
    "    }\n",
    "    return (\n",
    "        apply(\n",
    "            df,\n",
    "            MLPClassifier,\n",
    "            parameter_grid,\n",
    "            oversample=oversample,\n",
    "            feature_selection=feature_selection,\n",
    "        )\n",
    "        for oversample, feature_selection in (\n",
    "            (False, False),\n",
    "            (False, True),\n",
    "            (True, False),\n",
    "            (True, True),\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBOOST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_xgboost(df):\n",
    "    parameter_grid = {\n",
    "        \"min_child_weight\": range(1, 6, 2),\n",
    "        \"gamma\": [0, 0.2, 0.5, 1, 1.5, 3],\n",
    "        \"max_depth\": [5, 8, 10, 15],\n",
    "        \"reg_alpha\": [1e-5, 1e-2, 0.1, 1],\n",
    "    }\n",
    "    return (\n",
    "        apply(\n",
    "            df,\n",
    "            XGBClassifier,\n",
    "            parameter_grid,\n",
    "            oversample=oversample,\n",
    "            feature_selection=feature_selection,\n",
    "        )\n",
    "        for oversample, feature_selection in (\n",
    "            (False, False),\n",
    "            (False, True),\n",
    "            (True, False),\n",
    "            (True, True),\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "\n",
    "\n",
    "def compare_models(data):\n",
    "    scores = {\n",
    "        \"Decision Tree\": data[\"dt\"],\n",
    "        \"Logistic Regression\": data[\"lr\"],\n",
    "        \"Naive Bayes\": data[\"nb\"],\n",
    "        \"K-nearest Neighbours\": data[\"knn\"],\n",
    "        \"Random Forest\": data[\"rf\"],\n",
    "        \"xgboost\": data[\"xg\"],\n",
    "        \"Gradient Boosting\": data[\"gb\"],\n",
    "        \"SVC\": data[\"svc\"],\n",
    "    }\n",
    "\n",
    "    x_axis_labels = [\n",
    "        \"No Feature selection/No oversampling\",\n",
    "        \"Feature Selection\",\n",
    "        \"Oversampling\",\n",
    "        \"Feature Selection/Oversampling\",\n",
    "    ]\n",
    "    y_axis_labels = scores.keys()\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    scores_array = np.array([[score for score in model] for model in scores.values()])\n",
    "\n",
    "    sb.set(font_scale=1.3)\n",
    "    sb.heatmap(\n",
    "        scores_array,\n",
    "        annot=True,\n",
    "        linewidths=0.5,\n",
    "        vmax=1,\n",
    "        square=True,\n",
    "        xticklabels=x_axis_labels,\n",
    "        yticklabels=y_axis_labels,\n",
    "        cbar=False,\n",
    "    )\n",
    "    plt.title(\"ROC-AUC\")\n",
    "\n",
    "    plt.xticks(rotation=45, horizontalalignment=\"right\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" dfs = {\n",
    "  'Main': df, \n",
    "  #'Amount outliers removed': amount_outliers_df, \n",
    "  #'Age outliers removed': age_loan_outliers_df,\n",
    "  } \n",
    "\n",
    "\n",
    "models_comparison = {}\n",
    "\n",
    "for key, value in dfs.items():\n",
    "  display(Markdown(f\"# **{key} dataframe** \"))\n",
    "\n",
    "  display(Markdown(f\"## Random Forest \"))\n",
    "  rf, rf_fs, rf_os, rf_fs_os = apply_random_forest(value)\n",
    "  display(Markdown(\"---\"))\n",
    "\n",
    "  display(Markdown(f\"## XGBoost \"))\n",
    "  xg, xg_fs, xg_os, xg_fs_os = apply_xgboost(value)\n",
    "  display(Markdown(\"---\"))\n",
    "\n",
    "  display(Markdown(f\"## Decision Tree \"))\n",
    "  dt, dt_fs, dt_os, dt_fs_os = apply_decision_tree(value)\n",
    "  display(Markdown(\"---\"))\n",
    "\n",
    "  display(Markdown(f\"## Gradient Boosting \"))\n",
    "  gb, gb_fs, gb_os, gb_fs_os = apply_gradient_boosting(value)\n",
    "  display(Markdown(\"---\"))\n",
    "\n",
    "  display(Markdown(f\"## K Nearest Neighbours \"))\n",
    "  knn, knn_fs, knn_os, knn_fs_os = apply_k_nearest_neighbours(value)\n",
    "  display(Markdown(\"---\"))\n",
    "\n",
    "  display(Markdown(f\"## Logistic Regression \"))\n",
    "  lr, lr_fs, lr_os, lr_fs_os = apply_logistic_regression(value)\n",
    "  display(Markdown(\"---\"))\n",
    "\n",
    "  display(Markdown(f\"## Naive Bayes \"))\n",
    "  nb, nb_fs, nb_os, nb_fs_os = apply_naive_bayes(value)\n",
    "  display(Markdown(\"---\"))\n",
    "\n",
    "  display(Markdown(f\"## SVC \"))\n",
    "  svc, svc_fs, svc_os, svc_fs_os = apply_svc(value)\n",
    "  display(Markdown(\"---\"))\n",
    "\n",
    "  display(Markdown(f\"## MLP \"))\n",
    "  mlp, mlp_fs, mlp_os, mlp_fs_os = apply_mlp(value)\n",
    "  display(Markdown(\"---\"))\n",
    "  \n",
    "\n",
    "  models_comparison[key] = {\n",
    "                          'dt':[dt, dt_fs, dt_os, dt_fs_os],\n",
    "                          'gb':[gb, gb_fs, gb_os, gb_fs_os], \n",
    "                          'knn':[knn, knn_fs, knn_os, knn_fs_os], \n",
    "                          'lr':[lr, lr_fs, lr_os, lr_fs_os], \n",
    "                          'nb':[nb, nb_fs, nb_os, nb_fs_os], \n",
    "                          'xg':[xg, xg_fs, xg_os, xg_fs_os], \n",
    "                          'svc':[svc, svc_fs, svc_os, svc_fs_os],\n",
    "                          'mlp':[mlp, mlp_fs, mlp_os, mlp_fs_os],\n",
    "                          'rf':[rf, rf_fs, rf_os, rf_fs_os]} \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"## Random Forest \"))\n",
    "rf, rf_fs, rf_os, rf_fs_os = apply_random_forest(df)\n",
    "display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"## XGBoost \"))\n",
    "xg, xg_fs, xg_os, xg_fs_os = apply_xgboost(df)\n",
    "display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"## Decision Tree \"))\n",
    "dt, dt_fs, dt_os, dt_fs_os = apply_decision_tree(df)\n",
    "display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"## Gradient Boosting \"))\n",
    "gb, gb_fs, gb_os, gb_fs_os = apply_gradient_boosting(df)\n",
    "display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"## K Nearest Neighbours \"))\n",
    "knn, knn_fs, knn_os, knn_fs_os = apply_k_nearest_neighbours(df)\n",
    "display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"## Logistic Regression \"))\n",
    "lr, lr_fs, lr_os, lr_fs_os = apply_logistic_regression(df)\n",
    "display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"## Naive Bayes \"))\n",
    "nb, nb_fs, nb_os, nb_fs_os = apply_naive_bayes(df)\n",
    "display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"## SVC \"))\n",
    "svc, svc_fs, svc_os, svc_fs_os = apply_svc(df)\n",
    "display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"## MLP \"))\n",
    "mlp, mlp_fs, mlp_os, mlp_fs_os = apply_mlp(df)\n",
    "display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_comparison = {}\n",
    "models_comparison[\"main\"] = {\n",
    "    \"dt\": [dt, dt_fs, dt_os, dt_fs_os],\n",
    "    \"gb\": [gb, gb_fs, gb_os, gb_fs_os],\n",
    "    \"knn\": [knn, knn_fs, knn_os, knn_fs_os],\n",
    "    \"lr\": [lr, lr_fs, lr_os, lr_fs_os],\n",
    "    \"nb\": [nb, nb_fs, nb_os, nb_fs_os],\n",
    "    \"xg\": [xg, xg_fs, xg_os, xg_fs_os],\n",
    "    #'svc':[svc, svc_fs, svc_os, svc_fs_os],\n",
    "    \"mlp\": [mlp, mlp_fs, mlp_os, mlp_fs_os],\n",
    "    \"rf\": [rf, rf_fs, rf_os, rf_fs_os],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" display(Markdown(f\"# **Compare models** \"))\n",
    "\n",
    "for key in dfs.keys():\n",
    "  display(Markdown(f\"## {key} \"))\n",
    "\n",
    "  compare_models(models_comparison[key]) \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models(models_comparison[\"main\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
